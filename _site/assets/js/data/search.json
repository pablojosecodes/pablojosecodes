[
  
  {
    "title": "(textbook-style) Multivariable Calculus (In Progress)",
    "url": "/posts/(O)MVC/",
    "categories": "NLP, ML, Overview",
    "tags": "overview",
    "date": "2023-11-20 03:33:00 +0000",
    





    
    "snippet": "Welcome to my notes on Multivariable Calculus. These public notes are meant to be a resource for both current students and anyone interested in exploring the fascinating world of deep learning appl...",
    "content": "Welcome to my notes on Multivariable Calculus. These public notes are meant to be a resource for both current students and anyone interested in exploring the fascinating world of deep learning applied to language.This content should teach you the equivalent of an undergraduate course at a typical four-year college.These notes are based on the assumption that you have a solid understanding of the fundamentals of machine learning, including basic concepts in calculus, linear algebra, and probability. A familiarity with the basics of programming, particularly in Python, is also beneficial, as many examples and exercises will involve coding.Some important pointers for those using these materials to learn or review:      Exercises: I believe that the only thing which can effectively solidify knowledge is seeing concepts work in practice. As such, I have included exercises and sample projects for the reader to complete. I would advise at least attempting each of the exercises or understanding the source code (which I will post to Github).        Project: In addition to the exercises, I have included overarching project prompts which will solidify your learnings further. These should each take between 10-40 hours, so proceed with caution.        Layout: The sections do build upon each other sequentially, but I have done my best to modularize the learnings such that you can dive into any chapter of interest without missing too much context.  Here is a listing (and brief description) of the material that is in this set of notes:Intro to NLP: This chapter serves as an entry point into the world of Natural Language Processing (NLP) using machine learning techniques. It covers the basics of NLP, including the challenges and opportunities inherent in processing and understanding human language through computational means.      Distributional Semantics: In this section, we delve into the concept of distributional semantics, exploring how words can be represented in a way that captures their meanings based on the contexts they appear in. We also introduce word embeddings, a crucial technique in modern NLP that allows for efficient representation and processing of text data.        Sequential Models : This part of the notes focuses on Recurrent Neural Networks (RNNs), a class of neural networks particularly suited for sequential data like text. We cover the architecture of RNNs and how they can be used for various NLP tasks.        Advanced RNNs: Building on the previous chapter, we explore advanced designs of RNNs, including Long Short-Term Memory (LSTM) networks. This section also discusses how these architectures can be applied to solve complex NLP problems.        Transformers: This chapter introduces the Transformer model, a groundbreaking architecture in NLP. We cover the basics of how Transformers work, including their attention mechanism and how they differ from and improve upon previous sequential models.        Pretrained Language Models: Here, we explore the concept and application of pretrained language models in NLP, such as BERT and GPT. We discuss how these models are trained, their capabilities, and their impact on the field of NLP.        NLP Tools: This final section provides an overview of readily available tools and frameworks that can be used for NLP tasks. It’s a practical guide to applying NLP techniques without the need for building models from scratch, ideal for quick deployment and experimentation.  What are GNNs?: This introductory chapter provides an overview of Graph Neural Networks (GNNs), discussing why they are crucial in the data science field, how embeddings are applied in GNNs, and the basic architecture of these networks.      Layers: Focuses on the GNN layer, explaining how these layers process graph-structured data and how information is propagated within a GNN.        Connecting Layers: Covers various ways layers in a GNN can be connected, including different types of graph convolutions and their impact on learning.        Computation Graph: Provides an in-depth look at the computation graph of GNNs, explaining data flow and computations at each node and edge.        Augmentating GNNs: Explores data augmentation in GNNs, enhancing the model’s ability to learn robust representations from graph-structured data.        Training GNNs: Dedicated to the training process of GNNs, including loss functions, optimization techniques, and handling overfitting.        Relational GNNs: Delves into relational GNNs, designed for multi-relational data, particularly useful in scenarios with different types or properties of relationships.        Temporal GNNs: Discusses Temporal GNNs, specialized for handling graph data that changes over time, including unique challenges and techniques.  Knowledge Graphs: Introduces knowledge graphs, their structure, significance, and applications, and how GNNs can be applied to them.      Knowledge Graph Completion: Focuses on knowledge graph completion using GNNs, predicting missing information in a knowledge graph.        Knowledge Graph-Based GNN Applications: Explores various applications of GNNs in the context of knowledge graphs, demonstrating practical uses in real-world scenarios.  "
  },
  
  {
    "title": "(Conceptual) Calculus (in progress)",
    "url": "/posts/(O)calculus/",
    "categories": "NLP",
    "tags": "overview",
    "date": "2023-11-20 03:33:00 +0000",
    





    
    "snippet": "Welcome to my notes on Fundamental Concepts in Calculus, designed as a conceptual guide for students and enthusiasts alike. These notes aim to demystify calculus, making it more approachable and un...",
    "content": "Welcome to my notes on Fundamental Concepts in Calculus, designed as a conceptual guide for students and enthusiasts alike. These notes aim to demystify calculus, making it more approachable and understandable, especially for those who appreciate a visual and intuitive learning style. Since the emphasis is on the concepts, this guide is less authoritative and encompassing as my other posts, but should be helpful to build an intuition for Calculus regardless.Please note that these materials assume some basic knowledge in algebra and pre-calculus. A visual and geometric approach is often emphasized, making complex concepts more tangible.Here are some key elements to look out for:      Interactive Examples: To enhance understanding, I’ve included interactive examples and visualizations. These are crucial in grasping the more abstract aspects of calculus.        Chapter Summaries: Each chapter concludes with a summary to reinforce the key concepts and formulas.        Practical Applications: Wherever possible, I’ve related concepts to real-world applications. This helps in understanding the relevance and importance of calculus in various fields.  Now, let’s explore the chapters in this collection:The Essence of Calculus: This introductory chapter aims to capture the overarching ideas of calculus, using intuitive examples like rediscovering the formula for a circle’s area - a direct application of the fundamental theorem of calculus.The Paradox of the Derivative: A beginner-friendly explanation of derivatives, demystifying how they formalize seemingly paradoxical ideas.Power Rule through Geometry: Understanding the derivatives of polynomial terms with a focus on geometric intuition.Trig Derivatives through Geometry: Delve into the derivatives of trigonometric functions, explained in a visually intuitive manner.Visualizing the Chain Rule and Product Rule: Breaking down these fundamental rules of calculus into understandable, intuitive concepts.What’s so Special about Euler’s Number e?: Exploring the unique properties of e and its significance in calculus.Implicit Differentiation: A practical approach to understanding implicit differentiation in the context of functions with multiple inputs.Limits and the Definition of Derivatives: A foundational chapter that explores the concept of limits and how they are essential in defining derivatives.(ε, δ) “Epsilon Delta” Definitions of Limits: A more formal approach to understanding limits and their precise mathematical definitions.L’Hôpital’s Rule: Explaining L’Hôpital’s rule and its applications in evaluating limits.Integration and the Fundamental Theorem of Calculus: An introduction to integration and its connection to differentiation through the fundamental theorem of calculus.What Does Area Have to Do with Slope?: Linking the concept of area (integration) with the concept of slope (differentiation).Higher Order Derivatives: Understanding second, third, and higher-order derivatives and their interpretations.Taylor Series: A detailed exploration of Taylor series, their usefulness, and their underlying principles.Taylor Series (Geometric View): A geometric perspective on Taylor series, linking them to the fundamental theorem of calculus.The Other Way to Visualize Derivatives: A unique visual approach to understanding derivatives, thinking of functions as transformations and measuring local changes.By the end of these chapters, you should have a solid grasp of the fundamental concepts of calculus, equipped with both the theoretical knowledge and intuitive understanding needed to tackle more advanced topics or apply these concepts in practical scenarios."
  },
  
  {
    "title": "(Conceptual) Probability (in progress)",
    "url": "/posts/(O)Probability/",
    "categories": "Prob",
    "tags": "overview",
    "date": "2023-11-20 03:33:00 +0000",
    





    
    "snippet": "",
    "content": ""
  },
  
  {
    "title": "(Practically-minded) Natural Language Processing (in progress)",
    "url": "/posts/(O)NLP/",
    "categories": "NLP, ML",
    "tags": "overview",
    "date": "2023-11-20 03:33:00 +0000",
    





    
    "snippet": "Welcome to my notes on Deep Learning for Natural Language Processing, inspired by a Master’s course at the Polytechnic University of Madrid. These public notes are meant to be a resource for both c...",
    "content": "Welcome to my notes on Deep Learning for Natural Language Processing, inspired by a Master’s course at the Polytechnic University of Madrid. These public notes are meant to be a resource for both current students and anyone interested in exploring the fascinating world of deep learning applied to language.These notes are based on the assumption that you have a solid understanding of the fundamentals of machine learning, including basic concepts in calculus, linear algebra, and probability. A familiarity with the basics of programming, particularly in Python, is also beneficial, as many examples and exercises will involve coding.Some important pointers for those using these materials to learn or review:      Exercises: I believe that the only thing which can effectively solidify knowledge is seeing concepts work in practice. As such, I have included exercises and sample projects for the reader to complete. I would advise at least attempting each of the exercises or understanding the source code (which I will post to Github).        Project: In addition to the exercises, I have included overarching project prompts which will solidify your learnings further. These should each take between 10-40 hours, so proceed with caution.        Layout: The sections do build upon each other sequentially, but I have done my best to modularize the learnings such that you can dive into any chapter of interest without missing too much context.  Here is a listing (and brief description) of the material that is in this set of notes:Intro to NLP: This chapter serves as an entry point into the world of Natural Language Processing (NLP) using machine learning techniques. It covers the basics of NLP, including the challenges and opportunities inherent in processing and understanding human language through computational means.      Distributional Semantics: In this section, we delve into the concept of distributional semantics, exploring how words can be represented in a way that captures their meanings based on the contexts they appear in. We also introduce word embeddings, a crucial technique in modern NLP that allows for efficient representation and processing of text data.        Sequential Models : This part of the notes focuses on Recurrent Neural Networks (RNNs), a class of neural networks particularly suited for sequential data like text. We cover the architecture of RNNs and how they can be used for various NLP tasks.        Advanced RNNs: Building on the previous chapter, we explore advanced designs of RNNs, including Long Short-Term Memory (LSTM) networks. This section also discusses how these architectures can be applied to solve complex NLP problems.        Transformers: This chapter introduces the Transformer model, a groundbreaking architecture in NLP. We cover the basics of how Transformers work, including their attention mechanism and how they differ from and improve upon previous sequential models.        Pretrained Language Models: Here, we explore the concept and application of pretrained language models in NLP, such as BERT and GPT. We discuss how these models are trained, their capabilities, and their impact on the field of NLP.        NLP Tools: This final section provides an overview of readily available tools and frameworks that can be used for NLP tasks. It’s a practical guide to applying NLP techniques without the need for building models from scratch, ideal for quick deployment and experimentation.  What are GNNs?: This introductory chapter provides an overview of Graph Neural Networks (GNNs), discussing why they are crucial in the data science field, how embeddings are applied in GNNs, and the basic architecture of these networks.      Layers: Focuses on the GNN layer, explaining how these layers process graph-structured data and how information is propagated within a GNN.        Connecting Layers: Covers various ways layers in a GNN can be connected, including different types of graph convolutions and their impact on learning.        Computation Graph: Provides an in-depth look at the computation graph of GNNs, explaining data flow and computations at each node and edge.        Augmentating GNNs: Explores data augmentation in GNNs, enhancing the model’s ability to learn robust representations from graph-structured data.        Training GNNs: Dedicated to the training process of GNNs, including loss functions, optimization techniques, and handling overfitting.        Relational GNNs: Delves into relational GNNs, designed for multi-relational data, particularly useful in scenarios with different types or properties of relationships.        Temporal GNNs: Discusses Temporal GNNs, specialized for handling graph data that changes over time, including unique challenges and techniques.  Knowledge Graphs: Introduces knowledge graphs, their structure, significance, and applications, and how GNNs can be applied to them.      Knowledge Graph Completion: Focuses on knowledge graph completion using GNNs, predicting missing information in a knowledge graph.        Knowledge Graph-Based GNN Applications: Explores various applications of GNNs in the context of knowledge graphs, demonstrating practical uses in real-world scenarios.  "
  },
  
  {
    "title": "(Conceptual) Linear Algebra (in progress)",
    "url": "/posts/(O)Linear-Algebra/",
    "categories": "LA",
    "tags": "overview",
    "date": "2023-11-20 03:33:00 +0000",
    





    
    "snippet": "",
    "content": ""
  },
  
  {
    "title": "(Practically-minded) Computer Vision (in progress)",
    "url": "/posts/(O)Computer-Vision/",
    "categories": "CV",
    "tags": "overview",
    "date": "2023-11-20 03:33:00 +0000",
    





    
    "snippet": "",
    "content": ""
  },
  
  {
    "title": "Chapter 2.1 What are GNNs?",
    "url": "/posts/gnns/",
    "categories": "NLP, ML, DL4NLP",
    "tags": "chapter",
    "date": "2023-08-08 00:00:00 +0000",
    





    
    "snippet": "GraphsGraph Components  Objects: nodes, vertices (N)  Interactions: Arcs, links, edges (E)  System: graph, network (G(N,E))To define any graph  What are its nodes?  What are its edges?Terminology/T...",
    "content": "GraphsGraph Components  Objects: nodes, vertices (N)  Interactions: Arcs, links, edges (E)  System: graph, network (G(N,E))To define any graph  What are its nodes?  What are its edges?Terminology/Types of Graphs  Directed vs undirected          Sample Directed: Twitter Followers      Sample Undircted: Facebook Friends        Degree/Grado Number of adjacent nodes for a given node  Grado Medio: Average degree of graph’s nodes. Represents the density of a graph.  Heterogeneous Graphs: contains multiple types of nodes or links          Bipartite graphs: Nodes are divided into 2 categories and arcs go from one to another        Adjacency Matrix          Not symmetric if graph is directed      Essentially, fill out a matrix $A$ where $A_{ij}$ is 1 if there is a link from node $i$ to $j$ and 0 if not        List of Edges          List of the pairs of  connected nodes        Adjacency List          For each ndoe, indicate its neighbors (easy to search through)        Multigraph          Can have parallel edges (ie adjacency matrix would have the numbers 2+, not just 1s)        Self-Edges          Can have edge to itself        Connectivity: more relations between all graph’s pairs of nodes          Strongly Connected Graph: Can travel to any vertex from any      Weakly connected graph: if would be strongly connected if directed edges were replaced with undirected ones.Node/Edge Attributes        Weight/Peso: frequency of communication between noes  Centrality/Centralidad: position/importance in the graph  Ranking: best friend, second best friend  Type: friend, acquaintance  Sign/Signo: friend/enemy, trusted/untrusted"
  },
  
  {
    "title": "Chapter 1.1 Distributional Semantics",
    "url": "/posts/Distributional-Semantics/",
    "categories": "NLP, ML, DL4NLP",
    "tags": "chapter",
    "date": "2023-08-08 00:00:00 +0000",
    





    
    "snippet": "Welcome to my notes on Deep Learning for Natural Language Processing, inspired by a Master’s course at the Polytechnic University of Madrid. These public notes are meant to be a resource for both c...",
    "content": "Welcome to my notes on Deep Learning for Natural Language Processing, inspired by a Master’s course at the Polytechnic University of Madrid. These public notes are meant to be a resource for both current students and anyone interested in exploring the fascinating world of deep learning applied to language.These notes are based on the assumption that you have a solid understanding of the fundamentals of machine learning, including basic concepts in calculus, linear algebra, and probability. A familiarity with the basics of programming, particularly in Python, is also beneficial, as many examples and exercises will involve coding.Some important pointers for those using these materials to learn or review:"
  }
  
]

